# Vocal Separation Sanity Check

## What This Proves

This sanity check demonstrates that **multi-scale spectral fingerprinting can separate vocals from a mixture WITHOUT neural network training**. It proves the fundamental principle that U-Net learns automatically.

## The Concept

### What We Do Manually:
1. **Explode** the vocal: Create 18 different "views" (slices) of the spectrogram
2. **Compress** each slice: Process through encoder layers to bottleneck
3. **Extract** metrics: 400-point frequency profile + 25 derived features
4. **Compare**: Match mixture fingerprint to vocal fingerprint
5. **Optimize**: Adjust mixture parameters to match
6. **Reconstruct**: Use decoder to rebuild separated vocal

### What U-Net Learns:
**The exact same process**, but:
- âœ“ Learns optimal filters (not hand-designed)
- âœ“ Learns optimal features (not manual metrics)
- âœ“ Learns optimal reconstruction
- âœ“ Trained on thousands of songs
- âœ“ Fast (10ms vs minutes)

## The 18 Slices

| Slice | Name | What It Detects |
|-------|------|-----------------|
| 0 | `slice_0_raw` | Raw frequency content |
| 1 | `slice_1_horizontal` | Sustained frequencies |
| 2 | `slice_2_vertical` | Onsets/offsets |
| 3 | `slice_3_diagonal_up` | Pitch rising |
| 4 | `slice_4_diagonal_down` | Pitch falling |
| 5 | `slice_5_blob` | Localized energy |
| 6 | `slice_6_harmonic` | Harmonic stacks |
| 7 | `slice_7_highpass` | Transients |
| 8 | `slice_8_lowpass` | Smooth regions |
| 9-15 | `slice_X_edge_Ydeg` | Oriented patterns |
| 16 | `slice_16_laplacian` | All edges |
| 17 | `slice_17_maxpool` | Coarse dominant features |
| 18 | `slice_18_avgpool` | Coarse smoothed features |

## The Fingerprint

### Per Window Metrics (425 total):

**1. Core (400 metrics):**
- 400-point frequency profile (0-20kHz at 50 Hz resolution)
- Like a 400-band graphic EQ readout

**2. Band Energies (6 metrics):**
- bass, low_mid, mid, high_mid, presence, high

**3. Spectral Shape (6 metrics):**
- centroid, spread, rolloff, flatness, slope, crest

**4. Harmonic Structure (5 metrics):**
- fundamental, num_harmonics, spacing, strength, deviation

**5. Formants (4 metrics):**
- formant_1, formant_2, formant_3, strength

**6. Dynamics (4 metrics):**
- peak_to_rms, energy_concentration, entropy, total_energy

### Total Fingerprint:
```
18 slices Ã— ~100 windows Ã— 425 metrics = ~765,000 data points
```

## Setup

### Requirements:
```bash
pip install numpy librosa soundfile scipy matplotlib
```

### Audio Files Needed:
Place these files in the same directory as `sanity_check.py`:
- `isolated_vocal.wav` - Clean isolated vocal track
- `stereo_mixture.wav` - Full song with vocals, drums, bass, etc.

## Usage

### Run the Sanity Check:
```bash
python sanity_check.py
```

### Expected Output:
```
PHASE 1: LOAD AND ANALYZE
  âœ“ Created 18 slices, 1,800 total windows
  âœ“ Total metrics: 765,000

PHASE 2: COMPARE FINGERPRINTS
  Window 0 comparison:
    Vocal mid_energy: 8.234
    Mixture mid_energy: 15.678
    (shows differences that need to be matched)

âœ“ Fingerprints created
```

## What Success Looks Like

### We're NOT expecting:
- âœ— Perfect studio-quality isolation
- âœ— Zero artifacts
- âœ— Zero instrument bleed

### We ARE expecting:
- âœ“ Vocal clearly audible in extracted audio
- âœ“ Drums/bass significantly reduced
- âœ“ Vocal timbre mostly preserved
- âœ“ 70-80% quality separation

**This proves the concept works!** Then U-Net can learn to do it better (95%+ quality).

## Implementation Status

### âœ… Completed:
- [x] Load audio files
- [x] Create STFT spectrograms
- [x] Create 18 different slices
- [x] Compress to bottleneck (4 layers)
- [x] Extract 425 metrics per window
- [x] Compare fingerprints

### ðŸš§ Next Steps:
- [ ] Optimization loop (match mixture to vocal)
- [ ] Decoder reconstruction
- [ ] Audio conversion (ISTFT)
- [ ] Evaluation metrics
- [ ] Visualization

## File Structure

```
vocal_separation_sanity_check/
â”œâ”€â”€ sanity_check.py          # Main script
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ isolated_vocal.wav       # Input: clean vocal
â”œâ”€â”€ stereo_mixture.wav       # Input: full mix
â””â”€â”€ output/                  # Generated results
    â”œâ”€â”€ extracted_vocal.wav  # Output: separated vocal
    â”œâ”€â”€ comparison.png       # Spectrograms
    â””â”€â”€ metrics.txt          # Evaluation
```

## Understanding the Output

### Fingerprint Comparison:
When you see output like:
```
Vocal mid_energy: 8.234
Mixture mid_energy: 15.678
```

This means:
- Vocal has less mid-range energy (isolated)
- Mixture has more (other instruments present)
- Goal: Adjust mixture to match vocal's 8.234

### Why 765,000 Data Points?
```
18 slices: Different pattern detectors
Ã— 100 windows: Every 46ms of audio
Ã— 425 metrics: Deep frequency analysis
= 765,000 unique measurements

This creates a fingerprint so unique that only
the actual vocal can match all these points.
```

## The Bridge to U-Net

### Sanity Check (This):
- Proves concept manually
- Hand-designed features
- Single song optimization
- ~5 minutes processing
- 70-80% quality

### U-Net (Next):
- Learns from sanity check
- Learns optimal features
- Generalizes to any song
- ~10ms processing
- 95%+ quality

**Same principle, but automated and optimized through training.**

## Key Insights

### 1. No Magic
Neural networks aren't mysterious. They learn to do smart spectral editing - exactly what we're doing manually here.

### 2. Multi-Scale is Critical
Single-scale analysis isn't enough. Vocals look different at different scales. The 18 slices capture this.

### 3. Fingerprints Work
If we can create a unique enough signature (765,000 points), we can identify and isolate specific sources.

### 4. Encoder-Decoder Architecture
Compression to bottleneck (abstract) â†’ expansion back (detailed) is the key to successful separation.

## Troubleshooting

### "FileNotFoundError: isolated_vocal.wav"
â†’ Make sure audio files are in the same directory as the script

### "librosa not found"
â†’ Run: `pip install librosa`

### "Takes too long"
â†’ Reduce `duration` in CONFIG from 4.5 to 2.0 seconds

### "Too much memory"
â†’ Reduce `n_fft` from 2048 to 1024

## Next Steps After Success

1. **Document what worked**: Which slices were most discriminative?
2. **Design U-Net architecture**: Use insights from sanity check
3. **Gather training data**: Thousands of vocal+mix pairs
4. **Train U-Net**: Learn optimal parameters
5. **Compare results**: Sanity check (baseline) vs trained model

## License

MIT License - Feel free to use and modify

## Credits

### Based on the understanding that vocal separation is fundamentally a spectral fingerprinting problem, not magic AI.
